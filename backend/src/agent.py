import json
import os
from openai import AsyncOpenAI
from .catalog import SyscoCatalog
from .models import LineItem

class ChefAgent:
  """
  The Core AI Logic / Orchestrator.
  
  Architecture:
  - Uses OpenAI's 'Function Calling' to bridge the gap between LLM reasoning and local data.
  - Implements a 'ReAct' (Reasoning + Acting) loop: The AI thinks, calls a tool (Search), 
    analyzes the result, and then generates the final JSON.
  """

  def __init__(self, catalog: SyscoCatalog):
    # Security: Ensure API keys are present before starting
    api_key = os.environ.get("OPENAI_API_KEY")
    if not api_key:
      raise ValueError("OPENAI_API_KEY environment variable is not set.")
        
    # We use AsyncOpenAI to handle multiple requests in parallel (batch processing)
    self.client = AsyncOpenAI(api_key=api_key)
    self.catalog = catalog

  async def estimate_item(self, menu_item: dict, learnings: str) -> LineItem:
    """
    Estimates the cost for a single menu item using RAG.
    """
      
    # --- PROMPT ENGINEERING: SCHEMA INJECTION ---
    # Instead of describing the JSON format in English (which is error-prone),
    # we inject the exact JSON Schema derived from our Pydantic Models.
    # This guarantees that the LLM's output will technically validate 99.9% of the time.
    schema_structure = json.dumps(LineItem.model_json_schema(), indent=2)

    system_prompt = f"""
    You are an expert Catering Estimator for 'Elegant Foods' (US West Coast).
    
    GLOBAL CONTEXT (Learnings from previous batches):
    {learnings}
    
    YOUR MISSION:
    1. Analyze the dish description.
    2. Break it down into specific ingredients.
    3. USE THE 'search_catalog' TOOL to find real pricing in Sysco.
    
    PRICING STRATEGY (CRITICAL):
    - PRIORITY 1: Sysco Catalog. If found, calculate unit cost from case price. Source = "sysco_catalog".
    - PRIORITY 2: Market Estimate. If NOT found in Sysco (e.g. Wagyu, Truffles), you MUST ESTIMATE the cost based on average US food service prices. Source = "estimated".
    - PRIORITY 3: Not Available. Only use this if the item is impossible to price. Source = "not_available".
    
    Do NOT return $0.00 or null unless absolutely necessary. The goal is to get a rough quote.
    
    CRITICAL OUTPUT RULES (STRICT JSON COMPLIANCE):
    - You MUST output a JSON object that strictly matches this schema:
    {schema_structure}
    
    - 'item_name': Must match the input menu name exactly.
    - 'unit_cost': Must be a NUMBER (float). Example: 5.50.
    - 'source': Must be exactly one of ["sysco_catalog", "estimated", "not_available"].
    """

    # --- TOOL DEFINITION (Function Calling) ---
    # This tells the LLM: "You have a search engine available. Use it."
    tools = [{
      "type": "function",
      "function": {
        "name": "search_catalog",
        "description": "Search the Sysco supplier catalog for an ingredient.",
        "parameters": {
          "type": "object",
          "properties": {
            "query": {"type": "string", "description": "Name of ingredient (e.g. 'heavy cream')"}
          },
          "required": ["query"]
        }
      }
    }]

    messages = [
      {"role": "system", "content": system_prompt},
      {"role": "user", "content": f"Estimate this item: {json.dumps(menu_item)}"}
    ]

    # --- STEP 1: REASONING PHASE ---
    # The model analyzes the request and decides if it needs to call a tool.
    # We use 'gpt-4.1' for high-fidelity instruction following.
    response = await self.client.chat.completions.create(
      model="gpt-4.1", 
      messages=messages,
      tools=tools,
      tool_choice="auto"
    )
    
    msg = response.choices[0].message
    messages.append(msg) # Maintain conversation history

    # --- STEP 2: ACTION PHASE (Tool Execution) ---
    if msg.tool_calls:
      for tool_call in msg.tool_calls:
        if tool_call.function.name == "search_catalog":
          # Parse arguments generated by the LLM
          args = json.loads(tool_call.function.arguments)
          
          # Execute the actual Python code (Search in DataFrame)
          search_results = self.catalog.search(args['query'])
          
          # Feed the real data back to the LLM
          messages.append({
            "role": "tool",
            "tool_call_id": tool_call.id,
            "content": json.dumps(search_results)
          })

      # --- STEP 3: SYNTHESIS PHASE ---
      # The LLM now has the user request + its own thought process + real data from tools.
      # It generates the final JSON response.
      final_response = await self.client.chat.completions.create(
        model="gpt-4.1",
        messages=messages,
        response_format={"type": "json_object"} # Force JSON mode
      )
      raw_json = final_response.choices[0].message.content
      
      # Validate strictly against Pydantic models
      try:
        return LineItem.model_validate_json(raw_json)
      except Exception as e:
        print(f"❌ JSON Validation Failed for {menu_item.get('name')}")
        print(f"Raw Output: {raw_json}")
        raise e
    
    # Fallback: If the model felt confident enough to answer without tools (rare but possible)
    try:
      return LineItem.model_validate_json(msg.content)
    except Exception as e:
      print(f"❌ JSON Validation Failed (No Tools) for {menu_item.get('name')}")
      raise e

  async def compact_context(self, batch_results: list) -> str:
    """
    Optimization: Context Compaction.
    Instead of keeping the entire chat history (which grows indefinitely and costs money),
    we summarize the 'Learnings' of each batch to pass to the next one.
    This solves the 'Lost in the Middle' phenomenon and keeps latency low.
    """
    summary_input = [
      {
        "item": r.item_name, 
        "missing_ingredients": [i.name for i in r.ingredients if i.source != 'sysco_catalog']
      }
      for r in batch_results
    ]
    
    response = await self.client.chat.completions.create(
      model="gpt-4.1",
      messages=[
        {"role": "system", "content": "Summarize new learnings about missing ingredients or catalog quirks in 2 sentences."},
        {"role": "user", "content": json.dumps(summary_input)}
      ]
    )
    return response.choices[0].message.content


  async def parse_menu_request(self, user_text: str) -> dict:
    """
    NLP Pipeline (Chat Mode).
    Converts unstructured natural language into the strict JSON format our backend expects.
    This enables the 'Chat Mode' feature in the UI.
    """
    system_prompt = """
    You are a Catering Menu Architect.
    Your goal is to convert unstructured user requests into a strictly structured JSON Menu Specification.
    
    OUTPUT FORMAT (Strict JSON):
    {
      "items": [
          {
            "name": "Dish Name",
            "description": "Detailed description (infer ingredients if vague)",
            "category": "appetizers" | "main_plates" | "desserts" | "cocktails"
          }
      ]
    }
    
    RULES:
    1. Infer details: If user says "Steak", output "Grilled Ribeye Steak with garlic butter...".
    2. Categorize logic: Assign the correct category based on the item type.
    3. Quantity: If the user mentions "for 50 people", ignore the count for now, just extract the menu items.
    """

    response = await self.client.chat.completions.create(
        model="gpt-4.1",
        messages=[
          {"role": "system", "content": system_prompt},
          {"role": "user", "content": user_text}
        ],
        response_format={"type": "json_object"}
    )
    
    return json.loads(response.choices[0].message.content)
